#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import random

import numpy as np
from keras.preprocessing import sequence
#from keras.utils import np_utils
import tensorflow as tf
from pre_process import maxlen,CONTENT_SEQ_LEN,DESC_SEQ_LEN,empty,eos,replaceUNK,index2word
from constants import seed
from keras.utils import np_utils
from generate_samples import lpadd
global graph,model
graph = tf.get_default_graph()





def flip_headline(x, nflips, model, debug, oov0,idx2word):
    """Flip some of the words in the second half (summary) with words predicted by the model."""
    ###print("FLIP_HEADLINE STARTED")
    if nflips is None or model is None or nflips <= 0:
        return x

    batch_size = len(x)
    try:
        assert np.all(x[:, CONTENT_SEQ_LEN] == eos) #Prathibha: Checks if all the sequences have 1 after CONTENT_SEQ_LEN
    except:
        pass
        ##print("Assertion in flipheadline")  
    with graph.as_default():
        probs = model.predict(x, verbose=0, batch_size=batch_size)
    

    
    """
    Generates output predictions for the input samples.
    Computation is done in batches.
    """
    x_out = x.copy()
    for b in range(batch_size):
        # pick locations we want to flip
        # 0...CONTENT_SEQ_LEN-1 are descriptions and should be fixed
        # CONTENT_SEQ_LEN is eos and should be fixed
        """
        Prathibha:
        CONTENT_SEQ_LEN+1 is the summary starting word
        debug=>false (passes in the training parameters)
        """
        flips = sorted(random.sample(range(CONTENT_SEQ_LEN + 1, maxlen), nflips)) #Prathibha: starting from summary first word till the end into n number of flips
        if debug and b < debug:
            continue
            ##print(b)
        for input_idx in flips: #Prathibha: After sorting input index should be either empty or eos
            if x[b, input_idx] == empty or x[b, input_idx] == eos:
                continue
            # convert from input location to label location
            # the output at CONTENT_SEQ_LEN (when input is eos) is feed as input at CONTENT_SEQ_LEN+1
            label_idx = input_idx - (CONTENT_SEQ_LEN + 1)
            prob = probs[b, label_idx]
            w = prob.argmax()
            if w == empty:  # replace accidental empty with oov
                w = oov0
            if debug and b < debug:
                continue
                ##print('{} => {}'.format(idx2word[x_out[b, input_idx]], idx2word[w]),)
            x_out[b, input_idx] = w
        if debug and b < debug:
            continue
            ##print()
    ###print("FLIP_HEADLINE END")        
    return x_out


def conv_seq_labels(xds, xhs, nflips, model, debug, oov0, vocab_size, nb_unknown_words, idx2word):
    """Convert description and headlines to padded input vectors; headlines are one-hot to label."""
    batch_size = len(xhs)
    ###print("CONV_SEQ_LABELS STARTED")
    try:
        
        assert len(xds) == batch_size
    except:
        pass
        ##print("Assertion in conv_seq_labels")    
    """
    Prathibha:
    oov0 for training is handled over here
    description words convertion
    """    
    x = [
        replaceUNK(lpadd(xd) + xh)
        for xd, xh in zip(xds, xhs)]  # the input does not have 2nd eos
    
    ##print("length of xh",x)
    """
    Prathibha:
    sequence.pad_sequences=>Pads sequences to the same length.
    return  Numpy array with shape (len(sequences), maxlen)
    If the sequence is shorter than maxlen, 0 is added at the end
    If the sequence is longer than maxlen, truncation happens at the end
    
    """
    
    x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')
    x = flip_headline(x, nflips=nflips, model=model, debug=debug, oov0=xds.count(2),  idx2word=index2word)
    """
    summary convertion
    """
    y = np.zeros((batch_size, DESC_SEQ_LEN, vocab_size))

    for i, xh in enumerate(xhs):
        xh = replaceUNK(xh)+ [eos] + [empty] * DESC_SEQ_LEN  # output does have a eos at end
        xh = xh[:DESC_SEQ_LEN]
        ##print("######################ERRORxh####################",xh)
        xh=xh.pop(-1)
        y[i, :, :] = np_utils.to_categorical(xh, vocab_size)
    ###print("CONV_SEQ_LABELS END")
    return x, y


def gen(Xd, Xh, batch_size, nb_batches, nflips, model, debug, vocab_size,  idx2word):
    """Yield batches.
    
    for training use nb_batches=None
    for validation generate deterministic results repeating every nb_batches
    Prathibha: Xd=>Xtrain , Xh=>Ytrain
    
    
    """
    ###print("GEN STARTED")
    # while training it is good idea to flip once in a while the values of the headlines from the
    # value taken from Xh to value generated by the model.
    c = nb_batches if nb_batches else 0
    while True:
        xds = []
        xhs = []
        if nb_batches and c >= nb_batches:
            c = 0
        new_seed = random.randint(0, 2e10)
        random.seed(c + 123456789 + seed)
        for b in range(batch_size): #This loop repeats for no of batches
            t = random.randint(0, len(Xd) - 1)

            xd = Xd[t]
            s = random.randint(min(CONTENT_SEQ_LEN, len(xd)), max(CONTENT_SEQ_LEN, len(xd))) #Prathibha: Either consider the words in content till CONTENT_SEQ_LEN or len(xd)
            xds.append(xd[:s]) #Prathibha: Append those into xds list

            xh = Xh[t]
            s = random.randint(min(DESC_SEQ_LEN, len(xh)), max(DESC_SEQ_LEN, len(xh))) #Prathibha: Either consider the words in summary till CONTENT_SEQ_LEN or len(xd)
            xhs.append(xh[:s]) #Prathibha: Append those into xhs list

        # undo the seeding before we yield inorder not to affect the caller
        c += 1
        random.seed(new_seed)
        ###print("GEN END")   
        yield conv_seq_labels(
            xds,
            xhs,
            nflips=nflips,
            model=model,
            debug=debug,
            oov0=xds.count(2),
         
            vocab_size=vocab_size,
            nb_unknown_words=xds.count(2),
            idx2word=index2word,
        )
